import streamlit as st
import boto3
from botocore.exceptions import ClientError
import base64
from io import BytesIO
from PIL import Image
import time
from utils import load_env_variables, get_bedrock_client, check_aws_credentials

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_env_variables()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Claude Sonnet ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = []

# AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
@st.cache_resource
def get_cached_bedrock_client():
    return get_bedrock_client()

# Claude Sonnet ëª¨ë¸ ID
MODEL_ID = "anthropic.claude-3-sonnet-20240229-v1:0"

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."

# í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜
def process_text_message(client, user_input):
    """
    í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    # ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
    user_message = {
        "role": "user",
        "content": [{"text": user_input}]
    }
    
    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.conversation_history.append(user_message)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompts = [{"text": SYSTEM_PROMPT}]
    
    # ì¶”ë¡  íŒŒë¼ë¯¸í„° ì„¤ì •
    inference_config = {"temperature": 0.7}
    additional_model_fields = {"top_k": 250}
    
    try:
        # Converse API í˜¸ì¶œ
        response = client.converse(
            modelId=MODEL_ID,
            messages=st.session_state.conversation_history,
            system=system_prompts,
            inferenceConfig=inference_config,
            additionalModelRequestFields=additional_model_fields
        )
        
        # ëª¨ë¸ ì‘ë‹µ ì¶”ì¶œ
        output_message = response['output']['message']
        
        # ëŒ€í™” ê¸°ë¡ì— ëª¨ë¸ ì‘ë‹µ ì¶”ê°€
        st.session_state.conversation_history.append(output_message)
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        response_text = ""
        for content in output_message['content']:
            if 'text' in content:
                response_text += content['text']
        
        return response_text
        
    except ClientError as err:
        message = err.response['Error']['Message']
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {message}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {message}"

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜
def process_text_message_streaming(client, user_input):
    """
    í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” í•¨ìˆ˜
    """
    # ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
    user_message = {
        "role": "user",
        "content": [{"text": user_input}]
    }
    
    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.conversation_history.append(user_message)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompts = [{"text": SYSTEM_PROMPT}]
    
    # ì¶”ë¡  íŒŒë¼ë¯¸í„° ì„¤ì •
    inference_config = {"temperature": 0.7}
    additional_model_fields = {"top_k": 250}
    
    try:
        # ConverseStream API í˜¸ì¶œ
        response = client.converse_stream(
            modelId=MODEL_ID,
            messages=st.session_state.conversation_history,
            system=system_prompts,
            inferenceConfig=inference_config,
            additionalModelRequestFields=additional_model_fields
        )
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        stream = response.get('stream')
        full_response = ""
        
        if stream:
            for event in stream:
                if 'contentBlockDelta' in event:
                    text_chunk = event['contentBlockDelta']['delta']['text']
                    full_response += text_chunk
                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸
                    yield full_response
        
        # ëŒ€í™” ê¸°ë¡ì— ëª¨ë¸ ì‘ë‹µ ì¶”ê°€
        assistant_message = {
            "role": "assistant",
            "content": [{"text": full_response}]
        }
        st.session_state.conversation_history.append(assistant_message)
        
        return full_response
        
    except ClientError as err:
        message = err.response['Error']['Message']
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {message}")
        yield f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {message}"

# ì´ë¯¸ì§€ ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜
def process_image_message(client, user_input, image):
    """
    ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    image_bytes = buffered.getvalue()
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
    user_message = {
        "role": "user",
        "content": [
            {"text": user_input},
            {
                "image": {
                    "format": "png",
                    "source": {
                        "bytes": image_bytes
                    }
                }
            }
        ]
    }
    
    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.conversation_history.append(user_message)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompts = [{"text": SYSTEM_PROMPT}]
    
    try:
        # Converse API í˜¸ì¶œ
        response = client.converse(
            modelId=MODEL_ID,
            messages=st.session_state.conversation_history,
            system=system_prompts
        )
        
        # ëª¨ë¸ ì‘ë‹µ ì¶”ì¶œ
        output_message = response['output']['message']
        
        # ëŒ€í™” ê¸°ë¡ì— ëª¨ë¸ ì‘ë‹µ ì¶”ê°€
        st.session_state.conversation_history.append(output_message)
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        response_text = ""
        for content in output_message['content']:
            if 'text' in content:
                response_text += content['text']
        
        return response_text
        
    except ClientError as err:
        message = err.response['Error']['Message']
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {message}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {message}"

# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜
def process_document_message(client, user_input, document):
    """
    ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    # ë¬¸ì„œ í˜•ì‹ ê°€ì ¸ì˜¤ê¸° ë° ë°”ì´íŠ¸ë¡œ ì½ê¸°
    document_format = document.name.split(".")[-1]
    document_bytes = document.getvalue()
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
    user_message = {
        "role": "user",
        "content": [
            {"text": user_input},
            {
                "document": {
                    "name": document.name,
                    "format": document_format,
                    "source": {
                        "bytes": document_bytes
                    }
                }
            }
        ]
    }
    
    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.conversation_history.append(user_message)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompts = [{"text": SYSTEM_PROMPT}]
    
    try:
        # Converse API í˜¸ì¶œ
        response = client.converse(
            modelId=MODEL_ID,
            messages=st.session_state.conversation_history,
            system=system_prompts
        )
        
        # ëª¨ë¸ ì‘ë‹µ ì¶”ì¶œ
        output_message = response['output']['message']
        
        # ëŒ€í™” ê¸°ë¡ì— ëª¨ë¸ ì‘ë‹µ ì¶”ê°€
        st.session_state.conversation_history.append(output_message)
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        response_text = ""
        for content in output_message['content']:
            if 'text' in content:
                response_text += content['text']
        
        return response_text
        
    except ClientError as err:
        message = err.response['Error']['Message']
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {message}")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {message}"

# AWS ìê²© ì¦ëª… í™•ì¸
aws_credentials_valid = check_aws_credentials()

# Streamlit UI êµ¬ì„±
st.title("ğŸ¤– Claude Sonnet ì±—ë´‡")
st.markdown("AWS Bedrockì˜ Claude Sonnet ëª¨ë¸ì„ í™œìš©í•œ ì±—ë´‡ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€/ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    
    # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì„¤ì •
    streaming_mode = st.checkbox("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”", value=True)
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.session_state.conversation_history = []
        st.success("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    st.markdown("### ì •ë³´")
    st.markdown("ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ AWS Bedrockì˜ Claude Sonnet ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    st.markdown("Anthropicì˜ Claude ëª¨ë¸ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# AWS ìê²© ì¦ëª…ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ê²½ê³  í‘œì‹œ
if not aws_credentials_valid:
    st.warning("AWS ìê²© ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.info("1. .env.example íŒŒì¼ì„ .envë¡œ ë³µì‚¬í•˜ì„¸ìš”.\n2. AWS ìê²© ì¦ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# íƒ­ ì„¤ì •
tab1, tab2 = st.tabs(["ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ë¬¸ì„œ ì—…ë¡œë“œ"])

# ì´ë¯¸ì§€ ì—…ë¡œë“œ íƒ­
with tab1:
    uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)", type=["png", "jpg", "jpeg"])
    if uploaded_image is not None:
        st.image(Image.open(uploaded_image), caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", width=400)

# ë¬¸ì„œ ì—…ë¡œë“œ íƒ­
with tab2:
    uploaded_document = st.file_uploader("ë¬¸ì„œ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)", type=["pdf", "txt", "docx"])
    if uploaded_document is not None:
        st.write(f"ì—…ë¡œë“œëœ ë¬¸ì„œ: {uploaded_document.name}")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ë¡œë”© í‘œì‹œ
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # Bedrock í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        client = get_cached_bedrock_client()
        
        # ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œëœ ê²½ìš°
        if uploaded_image is not None:
            message_placeholder.markdown("ì´ë¯¸ì§€ë¥¼ ë¶„ì„ ì¤‘...")
            response = process_image_message(client, prompt, Image.open(uploaded_image))
            message_placeholder.markdown(response)
        
        # ë¬¸ì„œê°€ ì—…ë¡œë“œëœ ê²½ìš°
        elif uploaded_document is not None:
            message_placeholder.markdown("ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘...")
            response = process_document_message(client, prompt, uploaded_document)
            message_placeholder.markdown(response)
        
        # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
        else:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°
            if streaming_mode:
                response = ""
                for response_chunk in process_text_message_streaming(client, prompt):
                    message_placeholder.markdown(response_chunk + "â–Œ")
                    time.sleep(0.01)
                message_placeholder.markdown(response_chunk)
                response = response_chunk
            # ì¼ë°˜ ëª¨ë“œ
            else:
                message_placeholder.markdown("ìƒê° ì¤‘...")
                response = process_text_message(client, prompt)
                message_placeholder.markdown(response)
    
    # ì‘ë‹µ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": response})